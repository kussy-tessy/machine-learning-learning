# 第6章　決定木
1. 百万個のインスタンスを持つ訓練セットで決定木を訓練するとき、適切な深さはどれくらいか。
 - そんなものCross Validationすればいいのでは。
 -(解答)m個の葉を持つよくバランスの取れた二分木の深さは、log(2,m)の端数を切り上げたものになる。二分決定木は、制限なしで訓練すると、訓練インスタンスごとにひとつの葉を持つことになり、訓練終了後ある程度平衡したものになる。そのため、訓練セットのインスタンスが100万個なら、決定木の深さはlog(2, 10**6)~=20になる。

2. ノードのジニ不純度は一般に親よりも高いか、低いか。それは一般にか、常にか。
 - 一般に低い。

3. 決定木が訓練セットに過学習している場合、max_depthを下げるとよいか。
 - よい。

4. 決定木が訓練セットに過小適合している場合、入力フィーチャーを増やすとよいか。
 - よい。
 - (解答)「決定木は訓練データのスケーリングやセンタリングの影響を受けない。そのため、過小適合している場合、入力特徴量を増やしても時間の無駄である」とあるが、前者と後者の論理関係が僕には分からなかった。

5. インスタンスが百万個ある訓練セットを対象として決定木を訓練するために1時間かかるとき、インスタンスが1千万個の訓練セットを対象として別の決定木を訓練するときにどれくらいの時間がかかるか。
 - 10時間
 - (解答)決定木の計算量はO(n×m log(m))であるので、m~=10**6なら11.7時間程度となる。

6. 訓練セットのインスタンスが10万個あるとき、presort=Trueを設定すると訓練のスピードは上がるか。
 - 上がりそう。知らんけど。
 - (解答)訓練セットのプレソートによって訓練のスピードが上がるのは、インスタンス数が数千個未満のときだけである。インスタンス数が10万個もあるなら、presort=Trueとすると訓練のスピードはかなり下がる。

7. 省略
8. 省略