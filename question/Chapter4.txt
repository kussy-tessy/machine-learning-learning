# 第4章　モデルの訓練
1. 数百万個もの特徴量を持つ訓練セットがあるときに使える線形回帰訓練アルゴリズムは何か。
- ロジスティック回帰
- (解答)確率的勾配降下法やミニバッチ勾配降下法が使える。メモリに乗り切るなら、バッチ勾配降下法もあり。正規方程式は、計算量があっという間に多くなり無理。

2. 訓練セットの特徴量のスケールがまちまちだとする。これによって悪影響を受けるアルゴリズムは何で、どのような影響があるか。その問題にはどのように対処すればよいか。
- ロジスティック回帰、SVMなど多数。正規化を行い、スケールを揃える。
-(解答)勾配降下法を使うときの収束の時間が長くなる。また、値の小さな特徴量が値の大きな特徴量に比べて無視されがちになる。正規方程式は正規化の必要ない。

3. ロジスティック回帰モデルを訓練しているときに、勾配降下法が局所的な最小値から抜け出せなくなることはあるか。
- なんかしらんけど非凸関数とかいうやつだったから、そういうことは起きない。
- (解答)コスト関数は凸関数である。

4. 十分な実行時間を与えれば、全ての勾配降下法アルゴリズムは同じモデルに帰着するか。
- 局所解に落ち込むことがあるので、そうとは限らない。
- 上記に加え、確率的DCやミニバッチBDは最適解の近傍を飛び回る。

5. バッチ勾配降下法を使っていて、エポックごとに検証誤差をプロットしているものとする。検証誤差が絶えず大きくなっていることに気づいた場合、何が起きていると考えられるか。この問題はどのように修正すればよいか。
- 学習率が高すぎるので下げる。

6. 検証誤差が上がりだしたときにミニバッチ勾配降下法をすぐに中止するのはよいことか。
- ミニバッチは毎回毎回確率的にインスタンスが選ばれるので、検証誤差が上がることはありうる。しかし中長期的にそのようなことが起きているのなら、過学習が起きているのかもしれない。その場合中止するのはよいことだ。

7. 本書で取り上げた勾配降下法アルゴリズムの中で、最適解の近辺に最も早く到達するのはどれか。それは実際に収束するか。ほかの勾配降下法も収束させるにはどうすればよいか。
- AdaGradとかいうやつとあともうひとつあった気がする。なんか知らんけど収束する。他の勾配降下法は確率に頼るとか……？
- (解答)（何か思っていた解答と違う）

8. 多項式回帰を使っているものとする。学習曲線をプロットしたところ、訓練誤差と検証誤差の間に大きな差があった。何が起きているのか。この問題を解決するための3つの方法は何か。
- 過学習が起きている。次数を下げるか、データをもっと増やすか、正則化する。

9. リッジ回帰を使っていて、訓練誤差と検証誤差がほとんど同じだが、非常に高いことに気が付いた。そのモデルが問題を起こしているのは、バイアスと分散どちらが高いからか。正則化パラメタのαを上げるべきか下げるべきか。
- バイアスが高い。αを下げる。

10. 次の理由を答えなさい。
- 線形回帰（正則化項なし）ではなくリッジ回帰を使うべき理由
- リッジ回帰ではなく、Lasso回帰を使うべき理由
- Lasso回帰ではなく、Elastic Netを使うべき理由

  - なんだったかな……。過学習を防ぎつつ、かといってバイアスが高すぎないようにする、みたいな。あと影響のない高次項を自然に無視するようなパラメタになるとか、そんな感じだった気がする。
  - (解答)正則化されたモデルは、正則化されていないモデルよりも性能が良いのでl2ペナルティを伴うリッジ回帰を使った方がよい（過学習を防げるから）。Lasso回帰はl1ペナルティのことであり、これは重みを0に引き下げる効果があり、最も重要な重みのみを残し他を0にして疎なモデルへ導く。実際に意味のある特徴量がわずかであると推測されるときに使う。しかし、Lasso回帰は、特徴量に相関があるときなどは不規則に振る舞うことがあるので、リッジ回帰との中間であるElastic Netはハイパーパラメタが一つ増えるものの性能が良い。

11. 写真を屋外/屋内、日中/夜間に分類したいものとする。ふたつのロジスティック回帰分類器を作るべきか、それともひとつのソフトマックス回帰分類器を作るべきか。
- メンテナンスもしやすそうだし、ふたつのロジスティック回帰。

12. 省略