# 第4章　サポートベクターマシン（SVM)
1. サポートベクトルマシンの基本的な考え方とは何か。
- d次元空間にプロットされたインスタンスたちをすぱっと分類するd-1次元の超平面を見つける。
- (解答)上記の理解で概ね合っているが、そのd-1次元の超平面には太さがある。つまり、できるだけ大きいマージンを確保することを目的とする。

2. サポートベクトルとは何か。
- 上記のd-1次元の超平面を張るために必要なd-1本のベクトル。
- (解答)上記の理解は全く違う。サポートベクトルとは、SVMを訓練した後に道の中に入るインスタンスのこと。サポートベクトルではないインスタンスは影響を持たない。そういったインスタンスはいくつ追加・削除・移動しても、決定境界は変わらない（ほんまか？）。

3. SVMを使うときに入力をスケーリングするのが重要なのはなぜか。
- 値の小さい特徴量があるとその近くを通る超平面を作るようになり、誤差が大きくなる。

4. SVM分類器は、インスタンスを分類するときに確信度のスコアを出力できるか。確率はどうか。
- 信頼度は超平面からの距離で出せると思う。確率は分からない。
- (解答)上記の解答で合っている。ただ確率に関しては、SVMのスコアをロジスティック回帰で確率に較正する機能がsk-learnには用意されている。

5. 数百の特徴量がある数百万のインスタンスによる訓練セットのモデルを訓練するために、SVMの主問題と双対問題のどちらを使うべきか。
- なんですかそれ。
- (解答)カーネル化SVMが使えるのは双対形式だけなので、この問いは線形SVMだけに当てはまる。SVM問題の主形式の計算量はO(m)なのに対し、双対形式の計算量はO(m^2)～O(m^3)である。インスタンス数が数百万もあるなら、双対形式は重過ぎるので主形式を使うべき。

6. RBFカーネルつきのSVM分類器を訓練したとする。訓練セットに過小適合しているように見える。γを増やすべきかそれとも減らすべきか。Cはどうか。
- γを増やす、Cも増やす。

7. できあいのQPソルバーを使ってソフトマージン線形SVM分類器の問題を解決するためには、QPパラメタ（H、f、A、B）をどのように設定するべきか。
- なんですかそれ。
- (解答)解答を読んでもカオスでよく分からなかった。

8. 省略

9. 省略

10. 省略